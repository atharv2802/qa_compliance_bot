# ============================================================================
# QA Compliance Bot - Environment Configuration
# ============================================================================
# Copy this file to .env and fill in your actual values
# Never commit .env to version control!

# ============================================================================
# Deployment Mode
# ============================================================================
# Set to 'local' for development/testing or 'production' for Render deployment
MODE=local

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Primary provider: openai, anthropic, or groq
LLM_PROVIDER=groq
LLM_MODEL=llama-3.1-8b-instant

# Fallback providers (comma-separated, optional)
# If primary fails, will try these in order
LLM_FALLBACK_PROVIDERS=openai

# Groq Configuration (primary - fast and free tier available)
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.1-8b-instant

# OpenAI Configuration (fallback - reliable and high quality)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini

# Anthropic Configuration (optional fallback - premium quality)
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_MODEL=claude-3-haiku-20240307

# ============================================================================
# LLM-as-a-Judge Configuration
# ============================================================================
# Judge model evaluates suggestion quality (should be different from primary)
# Recommended: Use a stronger model as judge (e.g., gpt-4o-mini when using Groq primary)
JUDGE_PROVIDER=openai
JUDGE_MODEL=gpt-4o-mini

# ============================================================================
# Data Configuration
# ============================================================================
DATA_DIR=./data
RUNS_DB=./data/qa_runs.duckdb

# ============================================================================
# API Configuration
# ============================================================================
# For local: use localhost
# For Render: use 0.0.0.0 (configured automatically based on MODE)
# Note: Render automatically provides PORT environment variable
API_HOST=0.0.0.0
# API_PORT is optional - defaults to 8000 (local) or 10000 (production)
# On Render, the PORT variable is used automatically
API_PORT=8000

# API URL for dashboard to connect to
# Local: http://localhost:8000
# Render: Your deployed API URL (e.g., https://your-api.onrender.com)
API_URL=http://localhost:8000

# ============================================================================
# Streamlit Configuration
# ============================================================================
STREAMLIT_PORT=8501

# ============================================================================
# A/B Testing
# ============================================================================
AB_TEST_BUCKET=on

# ============================================================================
# Logging
# ============================================================================
LOG_LEVEL=INFO

# ============================================================================
# CORS Configuration (for production)
# ============================================================================
# Comma-separated list of allowed origins (leave empty for '*' in development)
# Example: https://your-dashboard.onrender.com,https://your-app.com
CORS_ORIGINS=
