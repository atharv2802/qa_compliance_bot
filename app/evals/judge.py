"""
LLM-as-a-Judge evaluation system.

Uses a separate LLM model to evaluate the quality of suggestions
generated by the primary coach system.
"""

import os
import json
from typing import List, Dict, Optional
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()


@dataclass
class JudgeResponse:
    """Response from the judge evaluation."""
    overall_score: float  # 0-10 scale
    compliance_score: float  # 0-10 scale
    clarity_score: float  # 0-10 scale
    tone_score: float  # 0-10 scale
    completeness_score: float  # 0-10 scale
    feedback: str
    strengths: List[str]
    weaknesses: List[str]
    pass_threshold: bool  # True if overall_score >= 7.0
    

class Judge:
    """
    LLM-as-a-Judge for evaluating suggestion quality.
    
    Uses a separate model (typically stronger than the primary)
    to evaluate suggestions against multiple criteria.
    """
    
    def __init__(self):
        """Initialize the judge with configured provider."""
        self.provider_name = os.getenv("JUDGE_PROVIDER", "openai")
        self.model_name = os.getenv("JUDGE_MODEL", "gpt-4o-mini")
        self.provider = self._init_provider()
    
    def _init_provider(self):
        """Initialize the judge provider."""
        if self.provider_name == "openai":
            from app.providers.openai_provider import OpenAIProvider
            return OpenAIProvider()
        elif self.provider_name == "anthropic":
            from app.providers.anthropic_provider import AnthropicProvider
            return AnthropicProvider()
        elif self.provider_name == "groq":
            from app.providers.groq_provider import GroqProvider
            return GroqProvider()
        else:
            raise ValueError(f"Unknown judge provider: {self.provider_name}")
    
    def evaluate(
        self,
        agent_draft: str,
        suggestion: str,
        policy_refs: List[str],
        context: str = "",
        required_disclosures: Optional[List[str]] = None
    ) -> JudgeResponse:
        """
        Evaluate a suggestion using LLM-as-a-judge.
        
        Args:
            agent_draft: Original agent message (potentially non-compliant)
            suggestion: Coach's suggested rewrite
            policy_refs: List of policy IDs that were addressed
            context: Optional conversation context
            required_disclosures: Optional list of required disclosure phrases
        
        Returns:
            JudgeResponse with scores and feedback
        """
        prompt = self._build_judge_prompt(
            agent_draft=agent_draft,
            suggestion=suggestion,
            policy_refs=policy_refs,
            context=context,
            required_disclosures=required_disclosures
        )
        
        # Call the judge LLM
        response = self.provider.call_llm(
            prompt=prompt,
            model_override=self.model_name,
            temperature=0.3,  # Slightly higher for more nuanced evaluation
            max_tokens=800
        )
        
        # Parse the JSON response
        try:
            result = json.loads(response)
        except json.JSONDecodeError:
            # Fallback: try to extract JSON from markdown code blocks
            import re
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                # Return default failure response
                return JudgeResponse(
                    overall_score=0.0,
                    compliance_score=0.0,
                    clarity_score=0.0,
                    tone_score=0.0,
                    completeness_score=0.0,
                    feedback="Failed to parse judge response",
                    strengths=[],
                    weaknesses=["Judge returned malformed response"],
                    pass_threshold=False
                )
        
        # Build response
        overall = result.get("overall_score", 0.0)
        
        return JudgeResponse(
            overall_score=overall,
            compliance_score=result.get("compliance_score", 0.0),
            clarity_score=result.get("clarity_score", 0.0),
            tone_score=result.get("tone_score", 0.0),
            completeness_score=result.get("completeness_score", 0.0),
            feedback=result.get("feedback", ""),
            strengths=result.get("strengths", []),
            weaknesses=result.get("weaknesses", []),
            pass_threshold=overall >= 7.0
        )
    
    def _build_judge_prompt(
        self,
        agent_draft: str,
        suggestion: str,
        policy_refs: List[str],
        context: str,
        required_disclosures: Optional[List[str]]
    ) -> str:
        """Build the evaluation prompt for the judge."""
        
        disclosure_section = ""
        if required_disclosures:
            disclosure_section = f"""

**Required Disclosures:**
{chr(10).join(f"- {d}" for d in required_disclosures)}
"""
        
        prompt = f"""You are an expert evaluator for a QA compliance coaching system. Your job is to assess the quality of a suggested rewrite.

**Context:**
{context if context else "No additional context provided."}

**Original Agent Draft (potentially non-compliant):**
{agent_draft}

**Coach's Suggested Rewrite:**
{suggestion}

**Policy References Addressed:**
{", ".join(policy_refs) if policy_refs else "None"}
{disclosure_section}

**Evaluation Criteria:**

Evaluate the suggestion on a 0-10 scale for each criterion:

1. **Compliance (0-10):** Does the suggestion address all policy violations? Are required disclosures included?
2. **Clarity (0-10):** Is the suggestion clear, well-structured, and easy to understand?
3. **Tone (0-10):** Does it maintain a professional, empathetic, and helpful tone?
4. **Completeness (0-10):** Does it preserve the original intent while fixing compliance issues?

**Output Format:**

Respond with ONLY a JSON object (no markdown, no explanation):

{{
  "overall_score": <float 0-10>,
  "compliance_score": <float 0-10>,
  "clarity_score": <float 0-10>,
  "tone_score": <float 0-10>,
  "completeness_score": <float 0-10>,
  "feedback": "<brief summary of evaluation>",
  "strengths": ["<strength 1>", "<strength 2>", ...],
  "weaknesses": ["<weakness 1>", "<weakness 2>", ...]
}}

**Guidelines:**
- A score of 7+ is considered "passing"
- Be objective and constructive
- Focus on practical improvements
- Consider real-world QA coaching scenarios

Now evaluate the suggestion:"""

        return prompt


# Singleton instance
_judge_instance = None


def get_judge() -> Judge:
    """Get the singleton judge instance."""
    global _judge_instance
    if _judge_instance is None:
        _judge_instance = Judge()
    return _judge_instance


def evaluate_suggestion(
    agent_draft: str,
    suggestion: str,
    policy_refs: List[str],
    context: str = "",
    required_disclosures: Optional[List[str]] = None
) -> JudgeResponse:
    """
    Convenience function to evaluate a suggestion.
    
    Args:
        agent_draft: Original agent message
        suggestion: Coach's suggested rewrite
        policy_refs: Policy IDs addressed
        context: Conversation context
        required_disclosures: Required disclosure phrases
    
    Returns:
        JudgeResponse with scores and feedback
    """
    judge = get_judge()
    return judge.evaluate(
        agent_draft=agent_draft,
        suggestion=suggestion,
        policy_refs=policy_refs,
        context=context,
        required_disclosures=required_disclosures
    )
